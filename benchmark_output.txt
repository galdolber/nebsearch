Reflection warning, nebsearch/core.cljc:542:49 - reference to field length can't be resolved.
Reflection warning, nebsearch/core.cljc:544:28 - reference to field length can't be resolved.

Dataset found! Size: 1.81 MB
Parsing Wikipedia abstracts...
Found 5000 articles
Capping dataset to 5000 documents (from 5000)

Running with capped dataset: 5000 documents

???????????????????????????????????????????????????????????????
           REAL WORLD BENCHMARK STARTING
???????????????????????????????????????????????????????????????

Dataset: 5000 total documents
Total text size: 1.58 MB
Initial batch: 2500 docs
Incremental adds: 2500 docs

???????????????????????????????????????????????????????????????
TEST 1: Initial Index Build (Disk Storage)
???????????????????????????????????????????????????????????????

  Build time:        286.48 ms
  Docs/sec:          8727
  Store time:        528.70 ms
  Index file size:   2.90 MB
  Compression ratio: 0.5x

???????????????????????????????????????????????????????????????
TEST 2: Incremental Adds (Real-time Ingestion)
???????????????????????????????????????????????????????????????

  Single docs (100x):  1.80 s total, 18.01 ms per add
  Micro batches (10): 5.23 s total, 104.66 ms per batch
  Medium batch (5K):   332.06 ms

???????????????????????????????????????????????????????????????
TEST 3: Search Performance
???????????????????????????????????????????????????????????????

  Cold cache (first-time searches):
    Average: 139.13 ?s

  Warm cache (repeated searches):
    Total (1000 searches): 8.91 ms
    Average per search:    8.91 ?s
    Queries per second:    112259

  Multi-word searches:
    Average: 115.15 ?s

???????????????????????????????????????????????????????????????
TEST 4: Memory Usage
???????????????????????????????????????????????????????????????

  Index string size:      0 B (empty for disk!)
  IDs map (estimated):    39.06 KB
  Boundaries (estimated): 58.59 KB
  Total RAM estimate:     97.66 KB
  JVM heap used:          19.32 MB


???????????????????????????????????????????????????????????????
                  BENCHMARK RESULTS SUMMARY
???????????????????????????????????????????????????????????????

Dataset: 5000 documents, 1.58 MB total text

INITIAL INDEX BUILD:
  Time:          286.48 ms
  Throughput:    8727 docs/sec
  Index size:    2.90 MB

INCREMENTAL ADDS:
  Single doc:    18.01 ms per add
  Batch (10):    104.66 ms per batch
  Batch (5K):    332.06 ms total

SEARCH PERFORMANCE:
  Cold cache:    139.13 ?s per search
  Warm cache:    8.91 ?s per search
  Throughput:    112259 queries/sec
  Multi-word:    115.15 ?s per search

SCALABILITY ASSESSMENT:
  Add 1M docs (incremental):  ~300.1 minutes
  Add 10M docs (incremental): ~3001.0 minutes
  Estimated index size (1M):  ~1.13 GB
  Estimated index size (10M): ~11.33 GB

? All operations completed successfully!
? Substring search working correctly
? Real-time ingestion capable
? Ready for production use at million+ document scale

???????????????????????????????????????????????????????????????
